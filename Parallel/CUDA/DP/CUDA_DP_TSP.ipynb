{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07d_WyH8nj9H",
        "outputId": "57edd20e-91f6-495d-c20f-c7c737214852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oRkzsInyWa",
        "outputId": "96fcbee1-068e-438a-a7e6-ea15e7c3af4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKLU-r77n0f9",
        "outputId": "cbdb640a-f7f1-44f6-9d97-f5841cb174bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmplfr_bqvv\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_D_zw1nEgT",
        "outputId": "681843f8-fa5a-4a07-d9ba-7de7b904f98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes: 4 Time: 0.000438426 seconds. Min Cost: 80\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <limits.h>\n",
        "#include <chrono>\n",
        "\n",
        "#define MAX_V 4  // Maximum number of nodes\n",
        "\n",
        "__device__ const int dist[MAX_V][MAX_V] = {\n",
        "    { 0, 10, 15, 20 },\n",
        "    { 10, 0, 35, 25 },\n",
        "    { 15, 35, 0, 30 },\n",
        "    { 20, 25, 30, 0 }\n",
        "};\n",
        "\n",
        "// Kernel to compute TSP\n",
        "__device__ int tspKernel(int mask, int pos, int *dp, int n) {\n",
        "    // Base case: if all cities have been visited\n",
        "    if (mask == ((1 << n) - 1)) {\n",
        "        return dist[pos][0]; // Return to the starting city (city 0)\n",
        "    }\n",
        "\n",
        "    int index = mask * n + pos; // Create a unique index for dp\n",
        "    if (dp[index] != -1) {\n",
        "        return dp[index]; // Return cached result if available\n",
        "    }\n",
        "\n",
        "    int ans = INT_MAX; // Initialize answer to maximum\n",
        "    for (int city = 0; city < n; city++) {\n",
        "        // If the city has not been visited\n",
        "        if ((mask & (1 << city)) == 0) {\n",
        "            int newAns = dist[pos][city] + tspKernel(mask | (1 << city), city, dp, n);\n",
        "            ans = min(ans, newAns); // Update answer\n",
        "        }\n",
        "    }\n",
        "\n",
        "    dp[index] = ans; // Cache the result\n",
        "    return ans; // Return computed result\n",
        "}\n",
        "\n",
        "// Kernel wrapper for launching TSP\n",
        "__global__ void tspLauncher(int *dp, int n, int *result) {\n",
        "    result[0] = tspKernel(1, 0, dp, n); // Start TSP with node 0 and mask 1\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = MAX_V;  // Number of cities\n",
        "\n",
        "    // Initialize DP table in host memory\n",
        "    int *dp = new int[(1 << n) * n];\n",
        "    std::fill(dp, dp + (1 << n) * n, -1);\n",
        "\n",
        "    int *d_dp, *d_result;\n",
        "    int h_result;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&d_dp, (1 << n) * n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, sizeof(int));\n",
        "\n",
        "    // Copy dp to device\n",
        "    cudaMemcpy(d_dp, dp, (1 << n) * n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Measure execution time\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Launch the TSP kernel\n",
        "    tspLauncher<<<1, 1>>>(d_dp, n, d_result);\n",
        "    cudaDeviceSynchronize(); // Wait for the kernel to finish\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(&h_result, d_result, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> duration = end - start;\n",
        "\n",
        "    std::cout << \"Nodes: \" << n << \" Time: \" << duration.count() << \" seconds. Min Cost: \" << h_result << \"\\n\";\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_dp);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    // Free host memory\n",
        "    delete[] dp;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gO5kNr-cCcEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d607033c-866b-4d0c-daee-e4e2bdde9b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes: 4 Time: 0.0976705 seconds. Cost: 99\n",
            "Nodes: 5 Time: 0.000285805 seconds. Cost: 217\n",
            "Nodes: 6 Time: 0.000461098 seconds. Cost: 204\n",
            "Nodes: 7 Time: 0.000904126 seconds. Cost: 166\n",
            "Nodes: 8 Time: 0.00195309 seconds. Cost: 171\n",
            "Nodes: 9 Time: 0.00485123 seconds. Cost: 213\n",
            "Nodes: 10 Time: 0.000644813 seconds. Cost: 132\n",
            "Nodes: 11 Time: 0.00134765 seconds. Cost: 217\n",
            "Nodes: 12 Time: 0.00309146 seconds. Cost: 130\n",
            "Nodes: 13 Time: 0.00718959 seconds. Cost: 166\n",
            "Nodes: 14 Time: 0.0171433 seconds. Cost: 142\n",
            "Nodes: 15 Time: 0.0396535 seconds. Cost: 120\n",
            "Nodes: 16 Time: 0.0935338 seconds. Cost: 174\n",
            "Nodes: 17 Time: 0.240396 seconds. Cost: 195\n",
            "Nodes: 18 Time: 0.612629 seconds. Cost: 167\n",
            "Nodes: 19 Time: 1.4945 seconds. Cost: 182\n",
            "Nodes: 20 Time: 4.15719 seconds. Cost: 165\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <limits>\n",
        "#include <chrono>\n",
        "#include <random>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define MAX 20  // Maximum number of nodes\n",
        "#define THREADS_PER_BLOCK 256\n",
        "#define GPU_MAX_NODES 9  // Maximum nodes for GPU computation\n",
        "\n",
        "// Error checking macro for CUDA calls\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) {\n",
        "    if (code != cudaSuccess) {\n",
        "        fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "        if (abort) exit(code);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to generate a random adjacency matrix\n",
        "void generateRandomAdjacencyMatrix(int nodes, int maxWeight, std::vector<std::vector<int>>& adj) {\n",
        "    std::random_device rd;\n",
        "    std::mt19937 gen(rd());\n",
        "    std::uniform_int_distribution<> dis(1, maxWeight);\n",
        "\n",
        "    adj.resize(nodes, std::vector<int>(nodes));\n",
        "    for (int i = 0; i < nodes; ++i) {\n",
        "        for (int j = 0; j < nodes; ++j) {\n",
        "            if (i == j) {\n",
        "                adj[i][j] = 0;  // Diagonal elements are zero (no self-loop)\n",
        "            } else {\n",
        "                adj[i][j] = dis(gen);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU implementation of TSP using dynamic programming\n",
        "long long tspCPU(int mask, int pos, const std::vector<std::vector<int>>& adj, std::vector<std::vector<long long>>& dp) {\n",
        "    int n = adj.size();\n",
        "    if (mask == (1 << n) - 1) {\n",
        "        return adj[pos][0]; // Return to the starting city\n",
        "    }\n",
        "\n",
        "    if (dp[mask][pos] != -1) {\n",
        "        return dp[mask][pos];\n",
        "    }\n",
        "\n",
        "    long long ans = std::numeric_limits<long long>::max();\n",
        "    for (int city = 0; city < n; city++) {\n",
        "        if ((mask & (1 << city)) == 0) {\n",
        "            long long newAns = adj[pos][city] + tspCPU(mask | (1 << city), city, adj, dp);\n",
        "            ans = std::min(ans, newAns);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return dp[mask][pos] = ans;\n",
        "}\n",
        "\n",
        "// Device function for TSP using dynamic programming\n",
        "__device__ long long tspDP(int mask, int pos, const int *adj, long long *dp, int n) {\n",
        "    if (mask == (1 << n) - 1) {\n",
        "        return adj[pos * n + 0]; // Return to the starting city\n",
        "    }\n",
        "\n",
        "    int index = mask * n + pos; // Create a unique index for dp\n",
        "    if (dp[index] != -1) {\n",
        "        return dp[index];\n",
        "    }\n",
        "\n",
        "    long long ans = LLONG_MAX;\n",
        "    for (int city = 0; city < n; city++) {\n",
        "        if ((mask & (1 << city)) == 0) {\n",
        "            long long newAns = (long long)adj[pos * n + city] + tspDP(mask | (1 << city), city, adj, dp, n);\n",
        "            ans = min(ans, newAns);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return dp[index] = ans;\n",
        "}\n",
        "\n",
        "// Kernel to initialize the DP table\n",
        "__global__ void initDP(long long *dp, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        dp[idx] = -1;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel to launch the TSP DP calculation\n",
        "__global__ void tspLauncher(int *adj, long long *dp, int n, long long *result) {\n",
        "    if (threadIdx.x == 0 && blockIdx.x == 0) {\n",
        "        result[0] = tspDP(1, 0, adj, dp, n); // Start TSP from node 0 with mask 1\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int maxWeight = 100;\n",
        "\n",
        "    for (int nodes = 4; nodes <= MAX; nodes++) {\n",
        "        std::vector<std::vector<int>> adj;\n",
        "        generateRandomAdjacencyMatrix(nodes, maxWeight, adj);\n",
        "\n",
        "        auto start = std::chrono::high_resolution_clock::now();\n",
        "        long long result;\n",
        "\n",
        "        if (nodes <= GPU_MAX_NODES) {\n",
        "            // GPU implementation\n",
        "            int *d_adj;\n",
        "            long long *d_dp, *d_result;\n",
        "            long long *h_result = new long long[1];\n",
        "\n",
        "            gpuErrchk(cudaMalloc((void**)&d_adj, nodes * nodes * sizeof(int)));\n",
        "            gpuErrchk(cudaMalloc((void**)&d_dp, (1LL << nodes) * nodes * sizeof(long long)));\n",
        "            gpuErrchk(cudaMalloc((void**)&d_result, sizeof(long long)));\n",
        "\n",
        "            // Copy adjacency matrix to device\n",
        "            std::vector<int> flat_adj(nodes * nodes);\n",
        "            for (int i = 0; i < nodes; ++i) {\n",
        "                for (int j = 0; j < nodes; ++j) {\n",
        "                    flat_adj[i * nodes + j] = adj[i][j];\n",
        "                }\n",
        "            }\n",
        "            gpuErrchk(cudaMemcpy(d_adj, flat_adj.data(), nodes * nodes * sizeof(int), cudaMemcpyHostToDevice));\n",
        "\n",
        "            // Initialize DP table\n",
        "            int dpSize = (1LL << nodes) * nodes;\n",
        "            int blocks = (dpSize + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "            initDP<<<blocks, THREADS_PER_BLOCK>>>(d_dp, dpSize);\n",
        "            gpuErrchk(cudaPeekAtLastError());\n",
        "            gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "            // Launch the TSP kernel\n",
        "            tspLauncher<<<1, 1>>>(d_adj, d_dp, nodes, d_result);\n",
        "            gpuErrchk(cudaPeekAtLastError());\n",
        "            gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "            // Copy result back to host\n",
        "            gpuErrchk(cudaMemcpy(h_result, d_result, sizeof(long long), cudaMemcpyDeviceToHost));\n",
        "\n",
        "            result = h_result[0];\n",
        "\n",
        "            // Free device memory\n",
        "            gpuErrchk(cudaFree(d_adj));\n",
        "            gpuErrchk(cudaFree(d_dp));\n",
        "            gpuErrchk(cudaFree(d_result));\n",
        "            delete[] h_result;\n",
        "        } else {\n",
        "            // CPU implementation\n",
        "            std::vector<std::vector<long long>> dp(1 << nodes, std::vector<long long>(nodes, -1));\n",
        "            result = tspCPU(1, 0, adj, dp);\n",
        "        }\n",
        "\n",
        "        auto end = std::chrono::high_resolution_clock::now();\n",
        "        std::chrono::duration<double> duration = end - start;\n",
        "\n",
        "        std::cout << \"Nodes: \" << nodes << \" Time: \" << duration.count() << \" seconds. Cost: \" << result << \"\\n\";\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkdcxJGXaOWN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}